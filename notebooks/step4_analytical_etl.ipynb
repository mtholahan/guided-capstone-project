{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c7abbbea-6f91-4a57-b68d-a3613edf318b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Shared/_init_azure_conn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28493d86-f539-49cb-bde0-0d27170ff633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Guided Capstone Step 4 â€“ Analytical ETL (Updated Template)\n",
    "# ============================================================\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Confirm ABFS access (optional quick check)\n",
    "display(dbutils.fs.ls(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/data/csv/\"))\n",
    "\n",
    "# (1) Spark Session and Imports\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder.appName(\"guided-step4-analytical-etl\").getOrCreate()\n",
    "\n",
    "# (2) Path Configuration\n",
    "# ------------------------------------------------------------\n",
    "# We reuse container_name and storage_account_name from _init_azure_conn\n",
    "base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "csv_path = f\"{base_path}/data/csv/*/*/*.txt\"\n",
    "json_path = f\"{base_path}/data/json/*/*/*.txt\"\n",
    "output_path = f\"{base_path}/output_dir/\"\n",
    "eod_dir = f\"{output_path}/eod\"\n",
    "analytical_dir = f\"{output_path}/analytical\"\n",
    "\n",
    "print(f\"âœ… Base path: {base_path}\")\n",
    "print(f\"ðŸ“¦ EOD path: {eod_dir}\")\n",
    "print(f\"ðŸ“Š Analytical path: {analytical_dir}\")\n",
    "\n",
    "# (3) Read Step 3 Outputs\n",
    "trade_df = spark.read.parquet(f\"{eod_dir}/trade\")\n",
    "quote_df = spark.read.parquet(f\"{eod_dir}/quote\")\n",
    "trade_df.createOrReplaceTempView(\"trades\")\n",
    "quote_df.createOrReplaceTempView(\"quotes\")\n",
    "\n",
    "# (4) Moving Average Calculation\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW tmp_trade_moving_avg AS\n",
    "SELECT\n",
    "    trade_dt,\n",
    "    symbol,\n",
    "    exchange,\n",
    "    CAST(event_tm AS TIMESTAMP) AS event_tm_ts,\n",
    "    event_seq_nb,\n",
    "    trade_pr,\n",
    "    AVG(trade_pr) OVER (\n",
    "        PARTITION BY symbol\n",
    "        ORDER BY CAST(event_tm AS TIMESTAMP)\n",
    "        RANGE BETWEEN INTERVAL 30 MINUTES PRECEDING AND CURRENT ROW\n",
    "    ) AS mov_avg_pr\n",
    "FROM trades\n",
    "\"\"\")\n",
    "\n",
    "# Safely write to Delta with schema evolution\n",
    "spark.table(\"tmp_trade_moving_avg\") \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"temp_trade_moving_avg\")\n",
    "\n",
    "spark.table(\"tmp_trade_moving_avg\").write.mode(\"overwrite\").saveAsTable(\"temp_trade_moving_avg\")\n",
    "\n",
    "# (5) Previous Day Last Trade\n",
    "\n",
    "# Base EOD path\n",
    "trade_path = f\"{eod_dir}/trade\"\n",
    "\n",
    "# Get all subdirectories under eod/trade (e.g., trade_dt=2020-07-29/)\n",
    "dirs = [f.name for f in dbutils.fs.ls(trade_path) if f.name.startswith(\"trade_dt=\")]\n",
    "\n",
    "if not dirs:\n",
    "    raise Exception(f\"No trade_dt partitions found under {trade_path}\")\n",
    "\n",
    "# Extract trade_dt values from folder names\n",
    "dates = [d.replace(\"trade_dt=\", \"\").replace(\"/\", \"\") for d in dirs]\n",
    "\n",
    "# Pick the latest available date\n",
    "latest_trade_dt = max(dates)\n",
    "processing_date = datetime.strptime(latest_trade_dt, \"%Y-%m-%d\")\n",
    "\n",
    "# Compute the prior date\n",
    "prev_date = (processing_date - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"âœ… Latest trade_dt detected: {latest_trade_dt}\")\n",
    "print(f\"ðŸ“… Previous trade_dt computed: {prev_date}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW tmp_last_trade AS\n",
    "SELECT\n",
    "    symbol,\n",
    "    exchange,\n",
    "    MAX(event_tm) AS last_trade_tm,\n",
    "    FIRST(trade_pr, TRUE) AS last_pr\n",
    "FROM trades\n",
    "WHERE trade_dt = '{prev_date}'\n",
    "GROUP BY symbol, exchange\n",
    "\"\"\")\n",
    "spark.table(\"tmp_last_trade\").write.mode(\"overwrite\").saveAsTable(\"temp_last_trade\")\n",
    "\n",
    "# (6) Analytical Joins\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW quote_union AS\n",
    "SELECT\n",
    "    trade_dt, symbol, exchange, event_tm, event_seq_nb,\n",
    "    'Q' AS rec_type,\n",
    "    bid_pr, bid_size, ask_pr, ask_size,\n",
    "    NULL AS trade_pr, NULL AS mov_avg_pr\n",
    "FROM quotes\n",
    "UNION ALL\n",
    "SELECT\n",
    "    trade_dt, symbol, exchange, event_tm_ts AS event_tm, event_seq_nb,\n",
    "    'T' AS rec_type,\n",
    "    NULL AS bid_pr, NULL AS bid_size, NULL AS ask_pr, NULL AS ask_size,\n",
    "    trade_pr, mov_avg_pr\n",
    "FROM temp_trade_moving_avg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW quote_union_update AS\n",
    "SELECT *,\n",
    "       LAST(trade_pr, TRUE) OVER (\n",
    "           PARTITION BY symbol, exchange ORDER BY event_tm\n",
    "           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "       ) AS last_trade_pr,\n",
    "       LAST(mov_avg_pr, TRUE) OVER (\n",
    "           PARTITION BY symbol, exchange ORDER BY event_tm\n",
    "           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "       ) AS last_mov_avg_pr\n",
    "FROM quote_union\n",
    "\"\"\")\n",
    "\n",
    "quote_update = spark.sql(\"\"\"\n",
    "SELECT trade_dt, symbol, exchange, event_tm, event_seq_nb,\n",
    "       bid_pr, bid_size, ask_pr, ask_size,\n",
    "       last_trade_pr, last_mov_avg_pr\n",
    "FROM quote_union_update\n",
    "WHERE rec_type = 'Q'\n",
    "\"\"\")\n",
    "quote_update.createOrReplaceTempView(\"quote_update\")\n",
    "\n",
    "# (7) Broadcast Join with Prior Day Close\n",
    "quote_final = spark.sql(\"\"\"\n",
    "SELECT /*+ BROADCAST(temp_last_trade) */\n",
    "    q.trade_dt, q.symbol, q.exchange, q.event_tm, q.event_seq_nb,\n",
    "    q.bid_pr, q.ask_pr, q.bid_size, q.ask_size,\n",
    "    q.last_trade_pr, q.last_mov_avg_pr,\n",
    "    (q.bid_pr - t.last_pr) AS bid_pr_mv,\n",
    "    (q.ask_pr - t.last_pr) AS ask_pr_mv\n",
    "FROM quote_update q\n",
    "LEFT JOIN temp_last_trade t\n",
    "ON q.symbol = t.symbol AND q.exchange = t.exchange\n",
    "\"\"\")\n",
    "\n",
    "# *(8) Write Final Output\n",
    "trade_date = processing_date.strftime(\"%Y-%m-%d\")\n",
    "analytical_out_path = f\"{analytical_dir}/date={trade_date}\"\n",
    "\n",
    "quote_final.write.mode(\"overwrite\").parquet(analytical_out_path)\n",
    "print(f\"âœ… Analytical ETL results written to: {analytical_out_path}\")\n",
    "\n",
    "# (9) Optional Audit Summary\n",
    "print(\"=== Analytical ETL Summary ===\")\n",
    "print(f\"Trade Date:     {trade_date}\")\n",
    "print(f\"Previous Date:  {prev_date}\")\n",
    "print(f\"Output Path:    {analytical_out_path}\")\n",
    "print(f\"Record Count:   {quote_final.count():,}\")\n",
    "print(\"ðŸŽ¯ Step 4 complete â€” ready for Step 5 orchestration.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "step4_analytical_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
